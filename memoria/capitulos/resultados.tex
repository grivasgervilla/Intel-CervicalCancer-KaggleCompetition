\section{Presentación y discusión de resultados}

Los primeros experimentos que se realizaron fue empleando la técnica de \emp{\textit{learning from scratch}}, es decir, diseñar y entrenar una red desde cero. Para ello se empleó la red que se definía en un script de Kaggle en un \textit{kernel} de esta competición y que se enlaza en la bibliografía. Aunque debido a nuestra inexperiencia en el ámbito del \textit{deep learning} no supimos cómo mejorar nuestra red o hacer su entrenamiento más efectivo de cara a la clasificación, si que pudimos observar algunos resultados bastante interesantes y que se explican a continuación.\\

En esta fase hicimos pruebas tanto con las imágenes rescaladas a 256x256 como a 32x32, este último rescalado fue algo que se hizo para simplemente poder realizar unas primeras pruebas sin demasiado tiempo de cómputo, no obstante resultaron ser unos experimentos más valiosos de lo que esperábamos. Y es que como podemos ver en \autoref{soluciones} se obtuvieron mejores resultados trabajando con imágenes de 32x32 que con las de mayor tamaño.\\

Realmente nosotros esperábamos que los resultados fuesen peores, de hecho una de nuestras preocupaciones de cara a afrontar la práctica era tener que reescalar las imágenes a un tamaño más reducido si los tiempos de cómputo eran demasiado elevados, esta preocupación se debía a que pensamos que cuanto más pequeña hiciésemos la imagen más información estaríamos perdiendo. Realmente lo que creemos que está influyendo en estos resultados en la simplicidad de la red que estamos estuadiando.\\

Lo que puede estar ocurriendo es que al tener una red tan simple ésta no sea capaz de extraer todos los matices que puede haber en una imagen y que por tanto la red se esté \textit{saturando de información} y por consiguiente esté sobreaprendiendo, por esto estamos obteniendo estos resultados. De hecho esto se observa bien cuando intentamos usar además del conjunto de entrenamiento base las imágenes adiconales que se facilitan en Kaggle. Mientras que en el caso de trabajar con imágenes pequeñas se mejora el score obtenido para las imágenes de 256x256 la red se \textit{satura} aún más y el score empeora. Observemos que el \textit{accuracy} en el conjunto de validación (un 40\% del conjunto de entrenamiento) no parece dar mucha información, en la mayoría de casos es similar, y cuando parece que nuestra red está sobreaprendiendo, en la \textit{submission} 3, el resultado obtenido es mejor que en el caso de las imágenes de 256x256 con un \textit{val\_acc} menor.\\

Distinto es cuando pasamos a hacer \emp{\textit{data augmentation}} siendo ahora la red entrenada con las imágens más grande la que mejores resultados obtiene, esto puede deberse a que al proporcionarla a la red, en cada \textit{epoch}, nuevos ejemplos conseguimos que el sobreaprendizaje que antes estábamos sufriendo se suavice pues ahora tiene más ejemplos sobre los que extraer características y por tanto puede realizar un aprendizaje más diversificado con su limitada capacidad por tratarse de un red relativamente simple. Si que vemos que cuando aumentamos el número de nuevas imágenes se generan, en la \textit{submission} 11, el resultado empeora ligéramente, quizás ya estemos nuevamente saturando la red al dar demasiados ejemplos que, por mucho que intentemos modificarlos a través de transformaciones aleatorias, no dejan de ser imágenes del conjunto de train original.\\

Cuando pasamos al proceso de \emp{\textit{fine tunning}} observamos que mientras que Inception V3 no mejoraba los resultados obtenidos hasta el momento, aunque sí que se obtenía una puntuación similar a los mejores resultados obtenidos, fue ResNet50 la red que mejoró nuestro resultados significativamente. Esto no quiere decir nada, de hecho no tenemos el conocimiento necesario como para poder analizar por qué con los mismo parámetros de entrenamiento una red da mejores resultados que otra. Lo que sí creemos es que al ser Inception una red de mayor tamaño (o al menos así nos lo ha parecido al obtener el número de capas de las redes implementadas en \code{Keras}) quizás necesite de más \textit{epochs} para poder aprender correctamente o que por otro lado al ser una red tan grande lo que estemos sufriendo sea de sobreaprendizaje, serían necesarios más experimentos para poder deducirlo.\\

Por otro lado, y dado que con ResNet50 obtuvimos nuestros mejores resultados hasta la fecha intentamos mejorarlos más entrenando la red durante más \textit{epochs}, para ello cargamos la última red que habíamos entrenado (gracias a guardar los pesos resultado del entrenamiento previo) y a partir de ahí entrenar la red durante 100 épocas más. Sin embargo los resultados fueron desastrosos producto, seguramente, del sobreaprendizaje. Por tanto aquí se nos plantea otra cuestión, ¿habría sido mejor usar como red para la predicción un de las etapas previas del entrenamiento de ResNet50 en lugar de usar la que mejor resultado de \textit{val\_acc} obtuvo?\\

Pasamos ahora a hablar de la técnica con la mejor resultados obtuvimos, \emp{la binarización del problema por medio de un esquema OVO}. Para ello, y a la luz de los resultados anteriores entrenamos (mediante nuevamente un proceso de \textit{fine tunning}) 3 ResNet50 que distinguiesen entre dos de las tres clases de nuestro problema. Una vez entrenadas y obtenidas las predicciones de cada una de ellas las combinamos mediante dos esquemas de agregación. Los mejores resultados fueron los obtenidos usando la media frente al esquema LVPC. La única razón que se nos ocurre de este resultado es que dado que el esquema LVPC penaliza a los clasificadores que sufren de incertidumbre en su decisión quizás lo que estemos haciendo sea \textit{obviar} una indecisión justificada y por tanto estemos equivocándonos al dar mayor peso a una clase que realmente no es la correcta.\\

Pasamos finalmente a los experimentos en los que aplicamos esquemas de aprendizaje automático como \emp{SVM o GBoost sobre características extraídas}. Desgraciadamente por cuestiones de tiempo que ya se han comentado anteriormente no se ha podido hacer una comparativa usando características extraídas con técnicas usuales del campo de la Visión por Computador, en nuestro caso hemos trabajado con características extraídas con la red neuronal (que distinguía entre las tres clases del problema), la ResNet50 reentrenada. Así hemos usado dos esquemas SVM y GBoost, siendo el primero el que mejores resultados de clasificación nos ha dado. Y observamos además que obtenemos peores resultados empleando los mejores parámetros dentro del \textit{grid} definido (que incluía los que se usaron en un primer intento) para GBoost que usando unos parámetros por defecto o al azar. Teóricamente Gradiente Boosting es una técnica que no es muy sensible al sobreaprendizaje (además que en el caso de SVM elegimos el parámetro que hace que nos ajustemos a los datos de entrenamiento lo máximo posible con el \textit{grid} de parámetros que definimos), sin embargo parece que al elegir el número más elevado de árboles de nuestro \textit{grid} y con la mayor profundidad posible esto ha podido hacer que nuestro modelo sobreaprenda. Además parece que las máquinas de soporte vectorial se han adaptado mejor a nuestro espacio de más de 2000 características que los árbol del gradient boosting.\\

Finalmente señalar que se han obtenido mejores resultados empleando CNNs directamente que realizando la extracción de características, si bien es cierto que estos resultados no son comparables puesto que no se ha realizado \textit{data augmentation} al extraer las características.





